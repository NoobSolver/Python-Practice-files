{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-d561023c2548>, line 485)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-d561023c2548>\"\u001b[1;36m, line \u001b[1;32m485\u001b[0m\n\u001b[1;33m    ))import os\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from multiprocessing import Pool, Manager\n",
    "from dateutil import parser as date_parser\n",
    "from chatterbot.conversation import Statement\n",
    "from chatterbot.tagging import PosHypernymTagger\n",
    "from chatterbot import utils\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Base class for all other trainer classes.\n",
    "    :param boolean show_training_progress: Show progress indicators for the\n",
    "           trainer. The environment variable ``CHATTERBOT_SHOW_TRAINING_PROGRESS``\n",
    "           can also be set to control this. ``show_training_progress`` will override\n",
    "           the environment variable if it is set.\n",
    "    :param str tagger_language: The language that the tagger uses to remove stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chatbot, **kwargs):\n",
    "        self.chatbot = chatbot\n",
    "\n",
    "        environment_default = os.getenv('CHATTERBOT_SHOW_TRAINING_PROGRESS', True)\n",
    "        self.show_training_progress = kwargs.get(\n",
    "            'show_training_progress',\n",
    "            environment_default\n",
    "        )\n",
    "\n",
    "        self.tagger = PosHypernymTagger(language=kwargs.get(\n",
    "            'tagger_language', 'english'\n",
    "        ))\n",
    "\n",
    "    def get_preprocessed_statement(self, input_statement):\n",
    "        \"\"\"\n",
    "        Preprocess the input statement.\n",
    "        \"\"\"\n",
    "        for preprocessor in self.chatbot.preprocessors:\n",
    "            input_statement = preprocessor(input_statement)\n",
    "\n",
    "        return input_statement\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        This method must be overridden by a child class.\n",
    "        \"\"\"\n",
    "        raise self.TrainerInitializationException()\n",
    "\n",
    "    class TrainerInitializationException(Exception):\n",
    "        \"\"\"\n",
    "        Exception raised when a base class has not overridden\n",
    "        the required methods on the Trainer base class.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, message=None):\n",
    "            default = (\n",
    "                'A training class must be specified before calling train(). '\n",
    "                'See http://chatterbot.readthedocs.io/en/stable/training.html'\n",
    "            )\n",
    "            super().__init__(message or default)\n",
    "\n",
    "    def _generate_export_data(self):\n",
    "        result = []\n",
    "        for statement in self.chatbot.storage.filter():\n",
    "            if statement.in_response_to:\n",
    "                result.append([statement.in_response_to, statement.text])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def export_for_training(self, file_path='./export.json'):\n",
    "        \"\"\"\n",
    "        Create a file from the database that can be used to\n",
    "        train other chat bots.\n",
    "        \"\"\"\n",
    "        import json\n",
    "        export = {'conversations': self._generate_export_data()}\n",
    "        with open(file_path, 'w+') as jsonfile:\n",
    "            json.dump(export, jsonfile, ensure_ascii=False)\n",
    "\n",
    "\n",
    "class ListTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows a chat bot to be trained using a list of strings\n",
    "    where the list represents a conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, conversation):\n",
    "        \"\"\"\n",
    "        Train the chat bot based on the provided list of\n",
    "        statements that represents a single conversation.\n",
    "        \"\"\"\n",
    "        previous_statement_text = None\n",
    "        previous_statement_search_text = ''\n",
    "\n",
    "        statements_to_create = []\n",
    "\n",
    "        for conversation_count, text in enumerate(conversation):\n",
    "            if self.show_training_progress:\n",
    "                utils.print_progress_bar(\n",
    "                    'List Trainer',\n",
    "                    conversation_count + 1, len(conversation)\n",
    "                )\n",
    "\n",
    "            statement_search_text = self.tagger.get_bigram_pair_string(text)\n",
    "\n",
    "            statement = self.get_preprocessed_statement(\n",
    "                Statement(\n",
    "                    text=text,\n",
    "                    search_text=statement_search_text,\n",
    "                    in_response_to=previous_statement_text,\n",
    "                    search_in_response_to=previous_statement_search_text,\n",
    "                    conversation='training'\n",
    "                )\n",
    "            )\n",
    "\n",
    "            previous_statement_text = statement.text\n",
    "            previous_statement_search_text = statement_search_text\n",
    "\n",
    "            statements_to_create.append(statement)\n",
    "\n",
    "        self.chatbot.storage.create_many(statements_to_create)\n",
    "\n",
    "\n",
    "class ChatterBotCorpusTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows the chat bot to be trained using data from the\n",
    "    ChatterBot dialog corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, *corpus_paths):\n",
    "        from chatterbot.corpus import load_corpus, list_corpus_files\n",
    "\n",
    "        data_file_paths = []\n",
    "\n",
    "        # Get the paths to each file the bot will be trained with\n",
    "        for corpus_path in corpus_paths:\n",
    "            data_file_paths.extend(list_corpus_files(corpus_path))\n",
    "\n",
    "        for corpus, categories, file_path in load_corpus(*data_file_paths):\n",
    "\n",
    "            statements_to_create = []\n",
    "\n",
    "            # Train the chat bot with each statement and response pair\n",
    "            for conversation_count, conversation in enumerate(corpus):\n",
    "\n",
    "                if self.show_training_progress:\n",
    "                    utils.print_progress_bar(\n",
    "                        'Training ' + str(os.path.basename(file_path)),\n",
    "                        conversation_count + 1,\n",
    "                        len(corpus)\n",
    "                    )\n",
    "\n",
    "                previous_statement_text = None\n",
    "                previous_statement_search_text = ''\n",
    "\n",
    "                for text in conversation:\n",
    "\n",
    "                    statement_search_text = self.tagger.get_bigram_pair_string(text)\n",
    "\n",
    "                    statement = Statement(\n",
    "                        text=text,\n",
    "                        search_text=statement_search_text,\n",
    "                        in_response_to=previous_statement_text,\n",
    "                        search_in_response_to=previous_statement_search_text,\n",
    "                        conversation='training'\n",
    "                    )\n",
    "\n",
    "                    statement.add_tags(*categories)\n",
    "\n",
    "                    statement = self.get_preprocessed_statement(statement)\n",
    "\n",
    "                    previous_statement_text = statement.text\n",
    "                    previous_statement_search_text = statement_search_text\n",
    "\n",
    "                    statements_to_create.append(statement)\n",
    "\n",
    "            self.chatbot.storage.create_many(statements_to_create)\n",
    "\n",
    "\n",
    "class TwitterTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows the chat bot to be trained using data\n",
    "    gathered from Twitter.\n",
    "    :param random_seed_word: The seed word to be used to get random tweets from the Twitter API.\n",
    "                             This parameter is optional. By default it is the word 'random'.\n",
    "    :param twitter_lang: Language for results as ISO 639-1 code.\n",
    "                         This parameter is optional. Default is None (all languages).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chatbot, **kwargs):\n",
    "        super().__init__(chatbot, **kwargs)\n",
    "        from twitter import Api as TwitterApi\n",
    "\n",
    "        # The word to be used as the first search term when searching for tweets\n",
    "        self.random_seed_word = kwargs.get('random_seed_word', 'random')\n",
    "        self.lang = kwargs.get('twitter_lang')\n",
    "\n",
    "        self.api = TwitterApi(\n",
    "            consumer_key=kwargs.get('twitter_consumer_key'),\n",
    "            consumer_secret=kwargs.get('twitter_consumer_secret'),\n",
    "            access_token_key=kwargs.get('twitter_access_token_key'),\n",
    "            access_token_secret=kwargs.get('twitter_access_token_secret')\n",
    "        )\n",
    "\n",
    "    def random_word(self, base_word, lang=None):\n",
    "        \"\"\"\n",
    "        Generate a random word using the Twitter API.\n",
    "        Search twitter for recent tweets containing the term 'random'.\n",
    "        Then randomly select one word from those tweets and do another\n",
    "        search with that word. Return a randomly selected word from the\n",
    "        new set of results.\n",
    "        \"\"\"\n",
    "        import random\n",
    "        random_tweets = self.api.GetSearch(term=base_word, count=5, lang=lang)\n",
    "        random_words = self.get_words_from_tweets(random_tweets)\n",
    "        random_word = random.choice(list(random_words))\n",
    "        tweets = self.api.GetSearch(term=random_word, count=5, lang=lang)\n",
    "        words = self.get_words_from_tweets(tweets)\n",
    "        word = random.choice(list(words))\n",
    "        return word\n",
    "\n",
    "    def get_words_from_tweets(self, tweets):\n",
    "        \"\"\"\n",
    "        Given a list of tweets, return the set of\n",
    "        words from the tweets.\n",
    "        \"\"\"\n",
    "        words = set()\n",
    "\n",
    "        for tweet in tweets:\n",
    "            tweet_words = tweet.text.split()\n",
    "\n",
    "            for word in tweet_words:\n",
    "                # If the word contains only letters with a length from 4 to 9\n",
    "                if word.isalpha() and len(word) > 3 and len(word) <= 9:\n",
    "                    words.add(word)\n",
    "\n",
    "        return words\n",
    "\n",
    "    def get_statements(self):\n",
    "        \"\"\"\n",
    "        Returns list of random statements from the API.\n",
    "        \"\"\"\n",
    "        from twitter import TwitterError\n",
    "        statements = []\n",
    "\n",
    "        # Generate a random word\n",
    "        random_word = self.random_word(self.random_seed_word, self.lang)\n",
    "\n",
    "        self.chatbot.logger.info('Requesting 50 random tweets containing the word {}'.format(random_word))\n",
    "        tweets = self.api.GetSearch(term=random_word, count=50, lang=self.lang)\n",
    "        for tweet in tweets:\n",
    "            statement = Statement(text=tweet.text)\n",
    "\n",
    "            if tweet.in_reply_to_status_id:\n",
    "                try:\n",
    "                    status = self.api.GetStatus(tweet.in_reply_to_status_id)\n",
    "                    statement.in_response_to = status.text\n",
    "                    statements.append(statement)\n",
    "                except TwitterError as error:\n",
    "                    self.chatbot.logger.warning(str(error))\n",
    "\n",
    "        self.chatbot.logger.info('Adding {} tweets with responses'.format(len(statements)))\n",
    "\n",
    "        return statements\n",
    "\n",
    "    def train(self):\n",
    "        for _ in range(0, 10):\n",
    "            statements = self.get_statements()\n",
    "            for statement in statements:\n",
    "                self.chatbot.storage.create(\n",
    "                    text=statement.text,\n",
    "                    in_response_to=statement.in_response_to,\n",
    "                    conversation=statement.conversation,\n",
    "                    tags=statement.tags\n",
    "                )\n",
    "\n",
    "\n",
    "def read_file(files, queue, preprocessors, tagger):\n",
    "\n",
    "    statements_from_file = []\n",
    "\n",
    "    for tsv_file in files:\n",
    "        with open(tsv_file, 'r', encoding='utf-8') as tsv:\n",
    "            reader = csv.reader(tsv, delimiter='\\t')\n",
    "\n",
    "            previous_statement_text = None\n",
    "            previous_statement_search_text = ''\n",
    "\n",
    "            for row in reader:\n",
    "                if len(row) > 0:\n",
    "                    statement = Statement(\n",
    "                        text=row[3],\n",
    "                        in_response_to=previous_statement_text,\n",
    "                        conversation='training',\n",
    "                        created_at=date_parser.parse(row[0]),\n",
    "                        persona=row[1]\n",
    "                    )\n",
    "\n",
    "                    for preprocessor in preprocessors:\n",
    "                        statement = preprocessor(statement)\n",
    "\n",
    "                    statement.search_text = tagger.get_bigram_pair_string(statement.text)\n",
    "                    statement.search_in_response_to = previous_statement_search_text\n",
    "\n",
    "                    previous_statement_text = statement.text\n",
    "                    previous_statement_search_text = statement.search_text\n",
    "\n",
    "                    statements_from_file.append(statement)\n",
    "\n",
    "    queue.put(tuple(statements_from_file))\n",
    "\n",
    "\n",
    "class UbuntuCorpusTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allow chatbots to be trained with the data from\n",
    "    the Ubuntu Dialog Corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chatbot, **kwargs):\n",
    "        super().__init__(chatbot, **kwargs)\n",
    "        home_directory = os.path.expanduser('~')\n",
    "\n",
    "        self.data_download_url = kwargs.get(\n",
    "            'ubuntu_corpus_data_download_url',\n",
    "            'http://cs.mcgill.ca/~jpineau/datasets/ubuntu-corpus-1.0/ubuntu_dialogs.tgz'\n",
    "        )\n",
    "\n",
    "        self.data_directory = kwargs.get(\n",
    "            'ubuntu_corpus_data_directory',\n",
    "            os.path.join(home_directory, 'ubuntu_data')\n",
    "        )\n",
    "\n",
    "        self.extracted_data_directory = os.path.join(\n",
    "            self.data_directory, 'ubuntu_dialogs'\n",
    "        )\n",
    "\n",
    "        # Create the data directory if it does not already exist\n",
    "        if not os.path.exists(self.data_directory):\n",
    "            os.makedirs(self.data_directory)\n",
    "\n",
    "    def is_downloaded(self, file_path):\n",
    "        \"\"\"\n",
    "        Check if the data file is already downloaded.\n",
    "        \"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            self.chatbot.logger.info('File is already downloaded')\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def is_extracted(self, file_path):\n",
    "        \"\"\"\n",
    "        Check if the data file is already extracted.\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.isdir(file_path):\n",
    "            self.chatbot.logger.info('File is already extracted')\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def download(self, url, show_status=True):\n",
    "        \"\"\"\n",
    "        Download a file from the given url.\n",
    "        Show a progress indicator for the download status.\n",
    "        Based on: http://stackoverflow.com/a/15645088/1547223\n",
    "        \"\"\"\n",
    "        import requests\n",
    "\n",
    "        file_name = url.split('/')[-1]\n",
    "        file_path = os.path.join(self.data_directory, file_name)\n",
    "\n",
    "        # Do not download the data if it already exists\n",
    "        if self.is_downloaded(file_path):\n",
    "            return file_path\n",
    "\n",
    "        with open(file_path, 'wb') as open_file:\n",
    "            print('Downloading %s' % url)\n",
    "            response = requests.get(url, stream=True)\n",
    "            total_length = response.headers.get('content-length')\n",
    "\n",
    "            if total_length is None:\n",
    "                # No content length header\n",
    "                open_file.write(response.content)\n",
    "            else:\n",
    "                download = 0\n",
    "                total_length = int(total_length)\n",
    "                for data in response.iter_content(chunk_size=4096):\n",
    "                    download += len(data)\n",
    "                    open_file.write(data)\n",
    "                    if show_status:\n",
    "                        done = int(50 * download / total_length)\n",
    "                        sys.stdout.write('\\r[%s%s]' % ('=' * done, ' ' * (50 - done)))\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "            # Add a new line after the download bar\n",
    "            sys.stdout.write('\\n')\n",
    "\n",
    "        print('Download location: %s' % file_path)\n",
    "        return file_path\n",
    "\n",
    "    def extract(self, file_path):\n",
    "        \"\"\"\n",
    "        Extract a tar file at the specified file path.\n",
    "        \"\"\"\n",
    "        import tarfile\n",
    "\n",
    "        print('Extracting {}'.format(file_path))\n",
    "\n",
    "        if not os.path.exists(self.extracted_data_directory):\n",
    "            os.makedirs(self.extracted_data_directory)\n",
    "\n",
    "        def track_progress(members):\n",
    "            sys.stdout.write('.')\n",
    "            for member in members:\n",
    "                # This will be the current file being extracted\n",
    "                yield member\n",
    "\n",
    "        with tarfile.open(file_path) as tar:\n",
    "            tar.extractall(path=self.extracted_data_directory, members=track_progress(tar))\n",
    "\n",
    "        self.chatbot.logger.info('File extracted to {}'.format(self.extracted_data_directory))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def train(self):\n",
    "        import glob\n",
    "\n",
    "        tagger = PosHypernymTagger(language=self.tagger.language)\n",
    "\n",
    "        # Download and extract the Ubuntu dialog corpus if needed\n",
    "        corpus_download_path = self.download(self.data_download_url)\n",
    "\n",
    "        # Extract if the directory does not already exist\n",
    "        if not self.is_extracted(self.extracted_data_directory):\n",
    "            self.extract(corpus_download_path)\n",
    "\n",
    "        extracted_corpus_path = os.path.join(\n",
    "            self.extracted_data_directory,\n",
    "            '**', '**', '*.tsv'\n",
    "        )\n",
    "\n",
    "        manager = Manager()\n",
    "        queue = manager.Queue()\n",
    "\n",
    "        def chunks(items, items_per_chunk):\n",
    "            for start_index in range(0, len(items), items_per_chunk):\n",
    "                end_index = start_index + items_per_chunk\n",
    "                yield items[start_index:end_index]\n",
    "\n",
    "        file_list = glob.glob(extracted_corpus_path)\n",
    "\n",
    "        file_groups = tuple(chunks(file_list, 10000))\n",
    "\n",
    "        argument_groups = tuple(\n",
    "            (\n",
    "                file_names,\n",
    "                queue,\n",
    "                self.chatbot.preprocessors,\n",
    "                tagger,\n",
    "            ) for file_names in file_groups\n",
    "        )\n",
    "\n",
    "        pool_batches = chunks(argument_groups, 9)\n",
    "\n",
    "        total_batches = len(file_groups)\n",
    "        batch_number = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        with Pool() as pool:\n",
    "            for pool_batch in pool_batches:\n",
    "                pool.starmap(read_file, pool_batch)\n",
    "\n",
    "                while True:\n",
    "\n",
    "                    if queue.empty():\n",
    "                        break\n",
    "\n",
    "                    batch_number += 1\n",
    "\n",
    "                    print('Training with batch {} with {} batches remaining...'.format(\n",
    "                        batch_number,\n",
    "                        total_batches - batch_number\n",
    "                    ))import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from multiprocessing import Pool, Manager\n",
    "from dateutil import parser as date_parser\n",
    "from chatterbot.conversation import Statement\n",
    "from chatterbot.tagging import PosHypernymTagger\n",
    "from chatterbot import utils\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Base class for all other trainer classes.\n",
    "    :param boolean show_training_progress: Show progress indicators for the\n",
    "           trainer. The environment variable ``CHATTERBOT_SHOW_TRAINING_PROGRESS``\n",
    "           can also be set to control this. ``show_training_progress`` will override\n",
    "           the environment variable if it is set.\n",
    "    :param str tagger_language: The language that the tagger uses to remove stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chatbot, **kwargs):\n",
    "        self.chatbot = chatbot\n",
    "\n",
    "        environment_default = os.getenv('CHATTERBOT_SHOW_TRAINING_PROGRESS', True)\n",
    "        self.show_training_progress = kwargs.get(\n",
    "            'show_training_progress',\n",
    "            environment_default\n",
    "        )\n",
    "\n",
    "        self.tagger = PosHypernymTagger(language=kwargs.get(\n",
    "            'tagger_language', 'english'\n",
    "        ))\n",
    "\n",
    "    def get_preprocessed_statement(self, input_statement):\n",
    "        \"\"\"\n",
    "        Preprocess the input statement.\n",
    "        \"\"\"\n",
    "        for preprocessor in self.chatbot.preprocessors:\n",
    "            input_statement = preprocessor(input_statement)\n",
    "\n",
    "        return input_statement\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        This method must be overridden by a child class.\n",
    "        \"\"\"\n",
    "        raise self.TrainerInitializationException()\n",
    "\n",
    "    class TrainerInitializationException(Exception):\n",
    "        \"\"\"\n",
    "        Exception raised when a base class has not overridden\n",
    "        the required methods on the Trainer base class.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, message=None):\n",
    "            default = (\n",
    "                'A training class must be specified before calling train(). '\n",
    "                'See http://chatterbot.readthedocs.io/en/stable/training.html'\n",
    "            )\n",
    "            super().__init__(message or default)\n",
    "\n",
    "    def _generate_export_data(self):\n",
    "        result = []\n",
    "        for statement in self.chatbot.storage.filter():\n",
    "            if statement.in_response_to:\n",
    "                result.append([statement.in_response_to, statement.text])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def export_for_training(self, file_path='./export.json'):\n",
    "        \"\"\"\n",
    "        Create a file from the database that can be used to\n",
    "        train other chat bots.\n",
    "        \"\"\"\n",
    "        import json\n",
    "        export = {'conversations': self._generate_export_data()}\n",
    "        with open(file_path, 'w+') as jsonfile:\n",
    "            json.dump(export, jsonfile, ensure_ascii=False)\n",
    "\n",
    "\n",
    "class ListTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows a chat bot to be trained using a list of strings\n",
    "    where the list represents a conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, conversation):\n",
    "        \"\"\"\n",
    "        Train the chat bot based on the provided list of\n",
    "        statements that represents a single conversation.\n",
    "        \"\"\"\n",
    "        previous_statement_text = None\n",
    "        previous_statement_search_text = ''\n",
    "\n",
    "        statements_to_create = []\n",
    "\n",
    "        for conversation_count, text in enumerate(conversation):\n",
    "            if self.show_training_progress:\n",
    "                utils.print_progress_bar(\n",
    "                    'List Trainer',\n",
    "                    conversation_count + 1, len(conversation)\n",
    "                )\n",
    "\n",
    "            statement_search_text = self.tagger.get_bigram_pair_string(text)\n",
    "\n",
    "            statement = self.get_preprocessed_statement(\n",
    "                Statement(\n",
    "                    text=text,\n",
    "                    search_text=statement_search_text,\n",
    "                    in_response_to=previous_statement_text,\n",
    "                    search_in_response_to=previous_statement_search_text,\n",
    "                    conversation='training'\n",
    "                )\n",
    "            )\n",
    "\n",
    "            previous_statement_text = statement.text\n",
    "            previous_statement_search_text = statement_search_text\n",
    "\n",
    "            statements_to_create.append(statement)\n",
    "\n",
    "        self.chatbot.storage.create_many(statements_to_create)\n",
    "\n",
    "\n",
    "class ChatterBotCorpusTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows the chat bot to be trained using data from the\n",
    "    ChatterBot dialog corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, *corpus_paths):\n",
    "        from chatterbot.corpus import load_corpus, list_corpus_files\n",
    "\n",
    "        data_file_paths = []\n",
    "\n",
    "        # Get the paths to each file the bot will be trained with\n",
    "        for corpus_path in corpus_paths:\n",
    "            data_file_paths.extend(list_corpus_files(corpus_path))\n",
    "\n",
    "        for corpus, categories, file_path in load_corpus(*data_file_paths):\n",
    "\n",
    "            statements_to_create = []\n",
    "\n",
    "            # Train the chat bot with each statement and response pair\n",
    "            for conversation_count, conversation in enumerate(corpus):\n",
    "\n",
    "                if self.show_training_progress:\n",
    "                    utils.print_progress_bar(\n",
    "                        'Training ' + str(os.path.basename(file_path)),\n",
    "                        conversation_count + 1,\n",
    "                        len(corpus)\n",
    "                    )\n",
    "\n",
    "                previous_statement_text = None\n",
    "                previous_statement_search_text = ''\n",
    "\n",
    "                for text in conversation:\n",
    "\n",
    "                    statement_search_text = self.tagger.get_bigram_pair_string(text)\n",
    "\n",
    "                    statement = Statement(\n",
    "                        text=text,\n",
    "                        search_text=statement_search_text,\n",
    "                        in_response_to=previous_statement_text,\n",
    "                        search_in_response_to=previous_statement_search_text,\n",
    "                        conversation='training'\n",
    "                    )\n",
    "\n",
    "                    statement.add_tags(*categories)\n",
    "\n",
    "                    statement = self.get_preprocessed_statement(statement)\n",
    "\n",
    "                    previous_statement_text = statement.text\n",
    "                    previous_statement_search_text = statement_search_text\n",
    "\n",
    "                    statements_to_create.append(statement)\n",
    "\n",
    "            self.chatbot.storage.create_many(statements_to_create)\n",
    "\n",
    "\n",
    "class TwitterTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows the chat bot to be trained using data\n",
    "    gathered from Twitter.\n",
    "    :param random_seed_word: The seed word to be used to get random tweets from the Twitter API.\n",
    "                             This parameter is optional. By default it is the word 'random'.\n",
    "    :param twitter_lang: Language for results as ISO 639-1 code.\n",
    "                         This parameter is optional. Default is None (all languages).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chatbot, **kwargs):\n",
    "        super().__init__(chatbot, **kwargs)\n",
    "        from twitter import Api as TwitterApi\n",
    "\n",
    "        # The word to be used as the first search term when searching for tweets\n",
    "        self.random_seed_word = kwargs.get('random_seed_word', 'random')\n",
    "        self.lang = kwargs.get('twitter_lang')\n",
    "\n",
    "        self.api = TwitterApi(\n",
    "            consumer_key=kwargs.get('twitter_consumer_key'),\n",
    "            consumer_secret=kwargs.get('twitter_consumer_secret'),\n",
    "            access_token_key=kwargs.get('twitter_access_token_key'),\n",
    "            access_token_secret=kwargs.get('twitter_access_token_secret')\n",
    "        )\n",
    "\n",
    "    def random_word(self, base_word, lang=None):\n",
    "        \"\"\"\n",
    "        Generate a random word using the Twitter API.\n",
    "        Search twitter for recent tweets containing the term 'random'.\n",
    "        Then randomly select one word from those tweets and do another\n",
    "        search with that word. Return a randomly selected word from the\n",
    "        new set of results.\n",
    "        \"\"\"\n",
    "        import random\n",
    "        random_tweets = self.api.GetSearch(term=base_word, count=5, lang=lang)\n",
    "        random_words = self.get_words_from_tweets(random_tweets)\n",
    "        random_word = random.choice(list(random_words))\n",
    "        tweets = self.api.GetSearch(term=random_word, count=5, lang=lang)\n",
    "        words = self.get_words_from_tweets(tweets)\n",
    "        word = random.choice(list(words))\n",
    "        return word\n",
    "\n",
    "    def get_words_from_tweets(self, tweets):\n",
    "        \"\"\"\n",
    "        Given a list of tweets, return the set of\n",
    "        words from the tweets.\n",
    "        \"\"\"\n",
    "        words = set()\n",
    "\n",
    "        for tweet in tweets:\n",
    "            tweet_words = tweet.text.split()\n",
    "\n",
    "            for word in tweet_words:\n",
    "                # If the word contains only letters with a length from 4 to 9\n",
    "                if word.isalpha() and len(word) > 3 and len(word) <= 9:\n",
    "                    words.add(word)\n",
    "\n",
    "        return words\n",
    "\n",
    "    def get_statements(self):\n",
    "        \"\"\"\n",
    "        Returns list of random statements from the API.\n",
    "        \"\"\"\n",
    "        from twitter import TwitterError\n",
    "        statements = []\n",
    "\n",
    "        # Generate a random word\n",
    "        random_word = self.random_word(self.random_seed_word, self.lang)\n",
    "\n",
    "        self.chatbot.logger.info('Requesting 50 random tweets containing the word {}'.format(random_word))\n",
    "        tweets = self.api.GetSearch(term=random_word, count=50, lang=self.lang)\n",
    "        for tweet in tweets:\n",
    "            statement = Statement(text=tweet.text)\n",
    "\n",
    "            if tweet.in_reply_to_status_id:\n",
    "                try:\n",
    "                    status = self.api.GetStatus(tweet.in_reply_to_status_id)\n",
    "                    statement.in_response_to = status.text\n",
    "                    statements.append(statement)\n",
    "                except TwitterError as error:\n",
    "                    self.chatbot.logger.warning(str(error))\n",
    "\n",
    "        self.chatbot.logger.info('Adding {} tweets with responses'.format(len(statements)))\n",
    "\n",
    "        return statements\n",
    "\n",
    "    def train(self):\n",
    "        for _ in range(0, 10):\n",
    "            statements = self.get_statements()\n",
    "            for statement in statements:\n",
    "                self.chatbot.storage.create(\n",
    "                    text=statement.text,\n",
    "                    in_response_to=statement.in_response_to,\n",
    "                    conversation=statement.conversation,\n",
    "                    tags=statement.tags\n",
    "                )\n",
    "\n",
    "\n",
    "def read_file(files, queue, preprocessors, tagger):\n",
    "\n",
    "    statements_from_file = []\n",
    "\n",
    "    for tsv_file in files:\n",
    "        with open(tsv_file, 'r', encoding='utf-8') as tsv:\n",
    "            reader = csv.reader(tsv, delimiter='\\t')\n",
    "\n",
    "            previous_statement_text = None\n",
    "            previous_statement_search_text = ''\n",
    "\n",
    "            for row in reader:\n",
    "                if len(row) > 0:\n",
    "                    statement = Statement(\n",
    "                        text=row[3],\n",
    "                        in_response_to=previous_statement_text,\n",
    "                        conversation='training',\n",
    "                        created_at=date_parser.parse(row[0]),\n",
    "                        persona=row[1]\n",
    "                    )\n",
    "\n",
    "                    for preprocessor in preprocessors:\n",
    "                        statement = preprocessor(statement)\n",
    "\n",
    "                    statement.search_text = tagger.get_bigram_pair_string(statement.text)\n",
    "                    statement.search_in_response_to = previous_statement_search_text\n",
    "\n",
    "                    previous_statement_text = statement.text\n",
    "                    previous_statement_search_text = statement.search_text\n",
    "\n",
    "                    statements_from_file.append(statement)\n",
    "\n",
    "    queue.put(tuple(statements_from_file))\n",
    "\n",
    "\n",
    "class UbuntuCorpusTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allow chatbots to be trained with the data from\n",
    "    the Ubuntu Dialog Corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chatbot, **kwargs):\n",
    "        super().__init__(chatbot, **kwargs)\n",
    "        home_directory = os.path.expanduser('~')\n",
    "\n",
    "        self.data_download_url = kwargs.get(\n",
    "            'ubuntu_corpus_data_download_url',\n",
    "            'http://cs.mcgill.ca/~jpineau/datasets/ubuntu-corpus-1.0/ubuntu_dialogs.tgz'\n",
    "        )\n",
    "\n",
    "        self.data_directory = kwargs.get(\n",
    "            'ubuntu_corpus_data_directory',\n",
    "            os.path.join(home_directory, 'ubuntu_data')\n",
    "        )\n",
    "\n",
    "        self.extracted_data_directory = os.path.join(\n",
    "            self.data_directory, 'ubuntu_dialogs'\n",
    "        )\n",
    "\n",
    "        # Create the data directory if it does not already exist\n",
    "        if not os.path.exists(self.data_directory):\n",
    "            os.makedirs(self.data_directory)\n",
    "\n",
    "    def is_downloaded(self, file_path):\n",
    "        \"\"\"\n",
    "        Check if the data file is already downloaded.\n",
    "        \"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            self.chatbot.logger.info('File is already downloaded')\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def is_extracted(self, file_path):\n",
    "        \"\"\"\n",
    "        Check if the data file is already extracted.\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.isdir(file_path):\n",
    "            self.chatbot.logger.info('File is already extracted')\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def download(self, url, show_status=True):\n",
    "        \"\"\"\n",
    "        Download a file from the given url.\n",
    "        Show a progress indicator for the download status.\n",
    "        Based on: http://stackoverflow.com/a/15645088/1547223\n",
    "        \"\"\"\n",
    "        import requests\n",
    "\n",
    "        file_name = url.split('/')[-1]\n",
    "        file_path = os.path.join(self.data_directory, file_name)\n",
    "\n",
    "        # Do not download the data if it already exists\n",
    "        if self.is_downloaded(file_path):\n",
    "            return file_path\n",
    "\n",
    "        with open(file_path, 'wb') as open_file:\n",
    "            print('Downloading %s' % url)\n",
    "            response = requests.get(url, stream=True)\n",
    "            total_length = response.headers.get('content-length')\n",
    "\n",
    "            if total_length is None:\n",
    "                # No content length header\n",
    "                open_file.write(response.content)\n",
    "            else:\n",
    "                download = 0\n",
    "                total_length = int(total_length)\n",
    "                for data in response.iter_content(chunk_size=4096):\n",
    "                    download += len(data)\n",
    "                    open_file.write(data)\n",
    "                    if show_status:\n",
    "                        done = int(50 * download / total_length)\n",
    "                        sys.stdout.write('\\r[%s%s]' % ('=' * done, ' ' * (50 - done)))\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "            # Add a new line after the download bar\n",
    "            sys.stdout.write('\\n')\n",
    "\n",
    "        print('Download location: %s' % file_path)\n",
    "        return file_path\n",
    "\n",
    "    def extract(self, file_path):\n",
    "        \"\"\"\n",
    "        Extract a tar file at the specified file path.\n",
    "        \"\"\"\n",
    "        import tarfile\n",
    "\n",
    "        print('Extracting {}'.format(file_path))\n",
    "\n",
    "        if not os.path.exists(self.extracted_data_directory):\n",
    "            os.makedirs(self.extracted_data_directory)\n",
    "\n",
    "        def track_progress(members):\n",
    "            sys.stdout.write('.')\n",
    "            for member in members:\n",
    "                # This will be the current file being extracted\n",
    "                yield member\n",
    "\n",
    "        with tarfile.open(file_path) as tar:\n",
    "            tar.extractall(path=self.extracted_data_directory, members=track_progress(tar))\n",
    "\n",
    "        self.chatbot.logger.info('File extracted to {}'.format(self.extracted_data_directory))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def train(self):\n",
    "        import glob\n",
    "\n",
    "        tagger = PosHypernymTagger(language=self.tagger.language)\n",
    "\n",
    "        # Download and extract the Ubuntu dialog corpus if needed\n",
    "        corpus_download_path = self.download(self.data_download_url)\n",
    "\n",
    "        # Extract if the directory does not already exist\n",
    "        if not self.is_extracted(self.extracted_data_directory):\n",
    "            self.extract(corpus_download_path)\n",
    "\n",
    "        extracted_corpus_path = os.path.join(\n",
    "            self.extracted_data_directory,\n",
    "            '**', '**', '*.tsv'\n",
    "        )\n",
    "\n",
    "        manager = Manager()\n",
    "        queue = manager.Queue()\n",
    "\n",
    "        def chunks(items, items_per_chunk):\n",
    "            for start_index in range(0, len(items), items_per_chunk):\n",
    "                end_index = start_index + items_per_chunk\n",
    "                yield items[start_index:end_index]\n",
    "\n",
    "        file_list = glob.glob(extracted_corpus_path)\n",
    "\n",
    "        file_groups = tuple(chunks(file_list, 10000))\n",
    "\n",
    "        argument_groups = tuple(\n",
    "            (\n",
    "                file_names,\n",
    "                queue,\n",
    "                self.chatbot.preprocessors,\n",
    "                tagger,\n",
    "            ) for file_names in file_groups\n",
    "        )\n",
    "\n",
    "        pool_batches = chunks(argument_groups, 9)\n",
    "\n",
    "        total_batches = len(file_groups)\n",
    "        batch_number = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        with Pool() as pool:\n",
    "            for pool_batch in pool_batches:\n",
    "                pool.starmap(read_file, pool_batch)\n",
    "\n",
    "                while True:\n",
    "\n",
    "                    if queue.empty():\n",
    "                        break\n",
    "\n",
    "                    batch_number += 1\n",
    "\n",
    "                    print('Training with batch {} with {} batches remaining...'.format(\n",
    "                        batch_number,\n",
    "                        total_batches - batch_number\n",
    "                    ))\n",
    "\n",
    "                    self.chatbot.storage.create_many(queue.get())\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "                time_per_batch = elapsed_time / batch_number\n",
    "                remaining_time = time_per_batch * (total_batches - batch_number)\n",
    "\n",
    "                print('{:.0f} hours {:.0f} minutes {:.0f} seconds elapsed.'.format(\n",
    "                    elapsed_time // 3600 % 24,\n",
    "                    elapsed_time // 60 % 60,\n",
    "                    elapsed_time % 60\n",
    "                ))\n",
    "\n",
    "                print('{:.0f} hours {:.0f} minutes {:.0f} seconds remaining.'.format(\n",
    "                    remaining_time // 3600 % 24,\n",
    "                    remaining_time // 60 % 60,\n",
    "                    remaining_time % 60\n",
    "                ))\n",
    "                print('---')\n",
    "\n",
    "print('Training took', time.time() - start_time, 'seconds.')\n",
    "\n",
    "                    self.chatbot.storage.create_many(queue.get())\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "                time_per_batch = elapsed_time / batch_number\n",
    "                remaining_time = time_per_batch * (total_batches - batch_number)\n",
    "\n",
    "                print('{:.0f} hours {:.0f} minutes {:.0f} seconds elapsed.'.format(\n",
    "                    elapsed_time // 3600 % 24,\n",
    "                    elapsed_time // 60 % 60,\n",
    "                    elapsed_time % 60\n",
    "                ))\n",
    "\n",
    "                print('{:.0f} hours {:.0f} minutes {:.0f} seconds remaining.'.format(\n",
    "                    remaining_time // 3600 % 24,\n",
    "                    remaining_time // 60 % 60,\n",
    "                    remaining_time % 60\n",
    "                ))\n",
    "                print('---')\n",
    "\n",
    "print('Training took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
